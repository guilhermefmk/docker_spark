{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"submitter\", StringType(), True),\n",
    "    StructField(\"authors\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"journal-ref\", StringType(), True),\n",
    "    StructField(\"doi\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"versions\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"JSON_RDD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 09:23:13 WARN Utils: Your hostname, guilherme-linux resolves to a loopback address: 127.0.1.1; using 192.168.2.154 instead (on interface enp5s0)\n",
      "24/10/04 09:23:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/04 09:23:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Cria uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .master(\"local[8]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"Read Json with .textFile with 100 partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_json = spark.sparkContext.textFile(\"/home/guilhermefmk/Documentos/labs_spark/data/arxivData.json\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.154:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JSON_RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4d86938610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"MAP RDD WITH json.dumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 09:23:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "rdd = rdd_json.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"PERSIST RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"RDD TAKE 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]},\n",
       " {'id': '0704.0002',\n",
       "  'submitter': 'Louis Theran',\n",
       "  'authors': 'Ileana Streinu and Louis Theran',\n",
       "  'title': 'Sparsity-certifying Graph Decompositions',\n",
       "  'comments': 'To appear in Graphs and Combinatorics',\n",
       "  'journal-ref': None,\n",
       "  'doi': None,\n",
       "  'report-no': None,\n",
       "  'categories': 'math.CO cs.CG',\n",
       "  'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n",
       "  'abstract': '  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'},\n",
       "   {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}],\n",
       "  'update_date': '2008-12-13',\n",
       "  'authors_parsed': [['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism) ## Número de partições padrão para um RDD quando não setado\n",
    "print(rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"RDD COUNT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2011231"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- authors_parsed: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- journal-ref: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- report-no: string (nullable = true)\n",
      " |-- submitter: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- update_date: string (nullable = true)\n",
      " |-- versions: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['comments',\n",
       " 'id',\n",
       " 'title',\n",
       " 'abstract',\n",
       " 'update_date',\n",
       " 'doi',\n",
       " 'categories',\n",
       " 'journal-ref',\n",
       " 'versions',\n",
       " 'authors',\n",
       " 'license',\n",
       " 'authors_parsed',\n",
       " 'report-no',\n",
       " 'submitter']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.keys()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 09:47:06 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4 (TID 103): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+---------+--------------------+--------------------+----------------+------------------+--------------------+-----------+--------------------+\n",
      "|            abstract|             authors|      authors_parsed|     categories|            comments|                 doi|       id|         journal-ref|             license|       report-no|         submitter|               title|update_date|            versions|\n",
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+---------+--------------------+--------------------+----------------+------------------+--------------------+-----------+--------------------+\n",
      "|  A fully differe...|C. Bal\\'azs, E. L...|[[Balázs, C., ], ...|         hep-ph|37 pages, 15 figu...|10.1103/PhysRevD....|0704.0001|Phys.Rev.D76:0130...|                NULL|ANL-HEP-PR-07-12|    Pavel Nadolsky|Calculation of pr...| 2008-11-26|[{version -> v1, ...|\n",
      "|  We describe a n...|Ileana Streinu an...|[[Streinu, Ileana...|  math.CO cs.CG|To appear in Grap...|                NULL|0704.0002|                NULL|http://arxiv.org/...|            NULL|      Louis Theran|Sparsity-certifyi...| 2008-12-13|[{version -> v1, ...|\n",
      "|  The evolution o...|         Hongjun Pan|  [[Pan, Hongjun, ]]| physics.gen-ph| 23 pages, 3 figures|                NULL|0704.0003|                NULL|                NULL|            NULL|       Hongjun Pan|The evolution of ...| 2008-01-13|[{version -> v1, ...|\n",
      "|  We show that a ...|        David Callan| [[Callan, David, ]]|        math.CO|            11 pages|                NULL|0704.0004|                NULL|                NULL|            NULL|      David Callan|A determinant of ...| 2007-05-23|[{version -> v1, ...|\n",
      "|  In this paper w...|Wael Abu-Shammala...|[[Abu-Shammala, W...|math.CA math.FA|                NULL|                NULL|0704.0005|Illinois J. Math....|                NULL|            NULL|Alberto Torchinsky|From dyadic $\\Lam...| 2013-10-15|[{version -> v1, ...|\n",
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+---------+--------------------+--------------------+----------------+------------------+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.toDF().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobGroup(\"JSON_RDD\", \"MAP FOR GET LICENSES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-sa/3.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-nd/4.0/',\n",
       " 'http://creativecommons.org/publicdomain/zero/1.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/4.0/',\n",
       " 'http://creativecommons.org/licenses/publicdomain/',\n",
       " 'http://creativecommons.org/licenses/by/3.0/',\n",
       " 'http://creativecommons.org/licenses/by/4.0/']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x[\"license\"]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shortest_title = rdd.reduce(lambda x, y: x if len(x[\"title\"]) < len(y[\"title\"]) else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "largest_title = rdd.reduce(lambda x, y: x if len(x[\"title\"]) > len(y[\"title\"]) else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Investigation of the 2-body system with a rotating central body (e. g.\\n  earth-moon system) within the Projective Unified Field theory: the transfer\\n  of rotational angular momentum and energy from the central body to the\\n  orbital 2-body system, the tidal and the non-tidal influences (mechanical,\\n  general-relativistic Lense-Thirring effect and cosmological\\n  PUFT-contributions)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_title[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_title[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(8, 138469),\n",
       " (1, 134247),\n",
       " (9, 138978),\n",
       " (10, 197755),\n",
       " (2, 116948),\n",
       " (11, 297963),\n",
       " (3, 126458),\n",
       " (12, 132305),\n",
       " (4, 117126),\n",
       " (5, 296587),\n",
       " (6, 191746),\n",
       " (7, 122649)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (datetime.strptime(x[\"update_date\"], \"%Y-%m-%d\").month, 1)).reduceByKey(lambda x,y: x+y).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
